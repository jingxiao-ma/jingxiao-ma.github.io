<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Dynamic Neural Network with Weight-Enabling Scheme | Jingxiao Ma </title> <meta name="author" content="Jingxiao Ma"> <meta name="description" content="A dynamic neural network framework that adaptively enables or disables weights during inference to balance accuracy and efficiency"> <meta name="keywords" content="deep learning, low-power computing, computer architecture"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/fig.jpg?483bd602ffda08e4bd472963bf73a8e4"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jingxiao-ma.github.io/projects/4_research.html"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jingxiao</span> Ma </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Dynamic Neural Network with Weight-Enabling Scheme</h1> <p class="post-description">A dynamic neural network framework that adaptively enables or disables weights during inference to balance accuracy and efficiency</p> </header> <article> <p>Paper <em>WeNet: Configurable Neural Network with Dynamic Weight-Enabling for Efficient Inference</em> <a class="citation" href="#ma2023wenet">(Ma &amp; Reda, 2023)</a> was published in the IEEE/ACM International Symposium on Low Power Electronics and Design (2023).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/wenet-480.webp 480w,/assets/img/wenet-800.webp 800w,/assets/img/wenet-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/wenet.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="intro" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Introduction of WeNet </div> <h3 id="motivation">Motivation</h3> <p>Deep Neural Networks (DNNs) have become indispensable for modern applications such as object detection, gesture recognition, and augmented reality. However, when deployed on resource-limited edge devices, these models face critical constraints in terms of computation, timing, and energy consumption. While training can be offloaded to cloud servers, inference often needs to run directly on edge devices, where computational resources vary drastically depending on device type, background workloads, and power budgets. This makes adaptability a central challenge: how can a single DNN model flexibly operate under diverse runtime conditions while maintaining a balance between accuracy, efficiency, and latency?</p> <h3 id="limitations-of-existing-approaches">Limitations of Existing Approaches</h3> <p>Prior research has introduced several categories of dynamic neural networks to address runtime efficiency:</p> <ul> <li>Flexible Width: Adjusts the number of active neurons per layer at runtime. Although this reduces computation, it risks discarding important features and losing accuracy.</li> <li>Flexible Depth: Introduces early exits to shorten inference paths. While this saves resources, additional branching structures can increase memory overhead.</li> <li>Flexible Precision: Dynamically lowers numerical precision to save energy. This preserves neuron count but requires specialized hardware for mixed-precision operations.</li> </ul> <p>While each strategy reduces inference cost, none provide a universal and fine-grained runtime control mechanism that is both hardware-friendly and accuracy-preserving.</p> <h3 id="wenet-a-new-direction">WeNet: A New Direction</h3> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/wenet_dense-480.webp 480w,/assets/img/wenet_dense-800.webp 800w,/assets/img/wenet_dense-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/wenet_dense.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="dense" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Illustration of WeNet on dense layer </div> <p>The WeNet framework (Weight-Enabling Network) introduces a new class of dynamic networks based on flexible weight-enabling:</p> <ul> <li>Instead of shrinking network width, depth, or precision, WeNet dynamically enables or disables subsets of weights during inference.</li> <li>Each enabled subset forms a sub-network that operates as a standalone model with specific accuracy–efficiency trade-offs.</li> <li>At runtime, edge devices can select the most suitable sub-network configuration depending on available resources, achieving significant adaptability without retraining or switching between multiple models.</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/wenet_conv-480.webp 480w,/assets/img/wenet_conv-800.webp 800w,/assets/img/wenet_conv-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/wenet_conv.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="conv" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Illustration of WeNet on convolution layer </div> <h3 id="key-innovations">Key Innovations</h3> <ul> <li>Dynamic Weight-Enabling: Layers are partitioned into independent groups where connections are selectively enabled. This ensures fewer computations while keeping the total number of neurons intact, preserving representational capacity better than width- or depth-based methods.</li> <li>Extension to Convolutional Layers: Through group convolution and channel shuffling, WeNet applies the same principle to CNNs. Channel shuffling ensures that reduced-weight sub-networks still exchange information across groups, mitigating accuracy loss.</li> <li>Training with Random Sub-Networks: During training, WeNet randomly samples sub-networks at each iteration and employs Switchable Batch Normalization (S-BN), ensuring all weight-enabling patterns are properly calibrated.</li> <li>Design Space Exploration (DSE): At inference time, WeNet applies an algorithm to explore the configuration space of sub-networks, finding Pareto-optimal operating points that balance accuracy, inference time, and energy consumption.</li> </ul> <h3 id="impact-and-evaluation">Impact and Evaluation</h3> <p>Experimental results show that:</p> <ul> <li>On benchmarks such as ResNet-50, MobileNet-V2, and EfficientNet-B0, WeNet delivers substantial improvements in energy efficiency and inference speed while retaining accuracy.</li> <li>Channel-shuffling boosts accuracy by nearly 3% on average with negligible overhead.</li> <li>Compared to US-Nets (universally slimmable networks), WeNet consistently achieves better trade-offs, especially when targeting resource-constrained hardware like the NVIDIA Jetson Nano.</li> <li>Across devices from high-performance GPUs to low-power CPUs, WeNet demonstrates flexible adaptability, making it suitable for a broad range of deployment environments.</li> </ul> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ISLPED</abbr> </div> <div id="ma2023wenet" class="col-sm-8"> <div class="title">WeNet: Configurable Neural Network with Dynamic Weight-Enabling for Efficient Inference</div> <div class="author"> <em>Jingxiao Ma</em> and Sherief Reda </div> <div class="periodical"> <em>In Proceedings of the IEEE/ACM International Symposium on Low Power Electronics and Design</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/ISLPED58423.2023.10244723" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://ieeexplore.ieee.org/abstract/document/10244723" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/wenet.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Deep Neural Networks (DNN) are widely deployed in resource-limited edge devices. Due to the limitation of computational resources, it is important to meet the timing and energy constraints while maintaining a high level of accuracy. To deploy the same DNN model on different edge devices, one challenge is to train a dynamic neural network with the flexibility of balancing the trade-off between accuracy and efficiency at runtime. In this paper, we present a novel methodology, dynamic Weight-enabling Network (WeNet), where the weights of neural network can be dynamically enabled or disabled to switch between different sub- networks, so that we are able to balance the trade-off between inference time, energy consumption and model accuracy. We extend the methodology to convolutional layers using group convolution and channel shuffling. We also propose a design space exploration approach to search for the optimal sub-network for different scenarios. We thoroughly evaluate our methodology using a number of DNN architectures on different hardware platforms, showing that WeNet provides a large number of energy-efficient operation modes, 73.2% of which provide better accuracy-efficiency trade-off compared to other methodologies.</p> </div> </div> </div> </li></ol> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Jingxiao Ma. Last updated: August 30, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>